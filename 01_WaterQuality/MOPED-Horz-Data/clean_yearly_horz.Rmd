1/13/2025

initial cleaning of horizontal WQ data

Title: Phycoprobe Data - Monthly Cleaning

By: Perry

```{r}
# ---
# EDIT THIS
# ---

# declare the run year of interest  
year <- 2020
```

```{r}
# ---
# CODE STARTS HERE
# ---

# import packages
library(tidyverse)
library(lubridate)
library(readxl)
library(zoo)
options(digits = 10)
source('03_Phyto/phycoprobe/functions/phyco_funcs.R')

# read in run names and regions
df_names <- read_csv('03_Phyto/phycoprobe/supp_files/run_names.csv', show_col_types = FALSE)

# obtain all file paths and create combo df
fp_all_wq <- archive_path(year, 'MOPED')

df_combo <- create_combo_df(fp_all_wq)

df_list <- list()
# df_rmlist <- list()

# run code for all combos
for(i in 1:nrow(df_combo)){
  # define variables
  month <- df_combo[i,]$month
  run <- df_combo[i,]$run
  
  print(glue::glue('month: {month} and run: {run}'))
  
  # read in WQ data
  fp_wq <- data_path(run, month, year, type = 'MOPED')
  
  df_wq <- read_csv(fp_wq,
                    skip = 2,
                    show_col_types = FALSE)

  df_wq <- df_wq %>% filter(Equipment == 'EXO Horizontal')
  df_wq$TimeStamp <- parse_date_time(df_wq$TimeStamp, c('mdY HMS', 'mdY HM'))
  df_wq$TimeStamp <- as.POSIXct(df_wq$TimeStamp, format = '%m/%d/%Y %H:%M:%S')
  
  df_wq$Analyte <- paste0(df_wq$Header,'_',df_wq$Unit)

  # remove rows where Longitude&Latitude OR TimeStamp repeat for a given analyte
  
  # df_removed <- df_wq %>%
  #   group_by(Header) %>%
  #   filter((Latitude %in% Latitude[duplicated(Latitude)] & 
  #           Longitude %in% Longitude[duplicated(Longitude)]) | 
  #          TimeStamp %in% TimeStamp[duplicated(TimeStamp)]) %>%
  #   ungroup()
  
  df_wq <- df_wq %>%
    group_by(Header) %>%
    filter(!((Latitude %in% Latitude[duplicated(Latitude)] & 
              Longitude %in% Longitude[duplicated(Longitude)]) | 
             TimeStamp %in% TimeStamp[duplicated(TimeStamp)])) %>%
    ungroup()

  df_wq <- df_wq %>%
    subset(select = c(Longitude, Latitude, TimeStamp, Analyte, Value)) %>%
    rename(DateTime = TimeStamp) %>%
    pivot_wider(
      names_from = Analyte,
      values_from = Value
    ) %>%
    filter(!if_any(everything(), ~ .x == 0 | is.na(.x)))

  ########Outlier Detection######
  
  # Add a column for the day
  df_wq$day <- as.Date(df_wq$DateTime)
  
  # Define the columns to run the outlier detection on
  columns_to_check <- c("WT_C", "SPC_uS/cm", "PH_pH Units", "NTU_NTU", "FLUOR_ug/L",  "FLUORRFU_RFU", "DO_mg/L" ,"DOSAT_% SAT")
  
  # Function to calculate rolling median and detect outliers for a single column
  detect_outliers <- function(column, window_size = 4) {
  rolling_median <- rollapply(column, width = window_size, FUN = median, fill = NA, align = "center")
  threshold <- 1.5 * IQR(column, na.rm = TRUE)
  outliers <- abs(column - rolling_median) > threshold
  return(outliers) # Return outlier flags
}
  
  # Apply outlier detection to the specified columns, for each day, creating new outlier result columns
  df_wq <- df_wq %>%
  group_by(day) %>% 
  mutate(across(all_of(columns_to_check), 
                ~ detect_outliers(.x), 
                .names = "{.col}_outlier")) %>% 
  ungroup()
  
  #replace NA with FALSE so it does not drop values
  df_wq[is.na(df_wq)] <- FALSE
  
  #Remove rows where cells equal TRUE
  df_wq <- subset(df_wq, WT_C_outlier != TRUE & 'SPC_uS/cm_outlier' != TRUE & 'PH_pH Units_outlier' != TRUE & NTU_NTU_outlier != TRUE & 'FLUOR_ug/L_outlier' != TRUE & FLUORRFU_RFU_outlier != TRUE & 'DO_mg/L_outlier' != TRUE & 'DOSAT_% SAT' != TRUE)
  
  #######
  
  df_wq <- df_wq %>%
    mutate(RoundedDateTime = round_date(DateTime, 'minute')) %>%
    mutate(TimeDiff = abs(as.numeric(difftime(DateTime, RoundedDateTime, units = 'secs')))) %>%
    group_by(RoundedDateTime) %>%
    slice_min(order_by = TimeDiff, n = 1, with_ties = FALSE) %>%
    ungroup() %>%
    select(-c(DateTime, TimeDiff)) %>%
    rename(DateTime = RoundedDateTime)
  
#  filter out rows around Antioch
  lat_min <- 38.016249
  lat_max <- 38.023349
  lon_min <- -121.759586
  lon_max <- -121.746626

  df_wq <- df_wq %>%
  filter(!(Latitude >= lat_min & Latitude <= lat_max &
             Longitude >= lon_min & Longitude <= lon_max))
  
  # filter        analytes
  # df_wq <- df_wq %>%
  #   filter(`SPC_uS/cm` > 50,
  #          WT_C > 0,
  #          `PH_pH Units` > 0,
  #          FNU_NTU > 0,
  #          `FLUOR_ug/L` > 0,
  #          FLUORRFU_RFU > 0,
  #          `DO_mg/L` > 0,
  #          `DOSAT_% SAT` > 0
  #          )

  # convert date/time col back to character
  df_wq$DateTime <- as.character(df_wq$DateTime)
  
  # bind to df list
  df_list[[i]] <- df_wq
  #df_rmlist[[i]] <- df_removed
}

# Combine all data frames into one
df_final <- bind_rows(df_list)
#df_remove <- bind_rows(df_rmlist)

df_final <- df_final %>%
  arrange(DateTime)

  df_final <- df_final %>%
    filter(`SPC_uS/cm` > 50,
           WT_C > 0,
           `PH_pH Units` > 0,
           NTU_NTU > 0,
           `FLUOR_ug/L` > 0,
           FLUORRFU_RFU > 0,
           `DO_mg/L` > 0,
           `DOSAT_% SAT` > 0
           )

# df_remove <- df_remove %>%
#   arrange(TimeStamp)
  
#Reorder columns and do not include chlorophyll RFU
df_final <- df_final %>% 
  select(DateTime, Longitude, Latitude, WT_C, `SPC_uS/cm`, `PH_pH Units`, NTU_NTU, `FLUOR_ug/L`, `DO_mg/L`, `DOSAT_% SAT`)

#export csv to Craig's desktop folder - change file name according to the year
write_csv(df_final, 'C:/Users/cstuart/Desktop/Annual Moped Files - Temp/MOPED_2019_combined_final_jul-dec_outliersremoved.csv')

#write_csv(df_final, 'C:/Users/sperry/Desktop/wq_test_19.csv')

# write_csv(df_remove, 'C:/Users/sperry/Desktop/wq_test_remove.csv')
```

```{r}
library(leaflet)
library(leaflet.extras)

# create leaflet maps
df_final$DateTime <- as.POSIXct(df_final$DateTime, format = '%Y-%m-%d %H:%M:%S')

create_map <- function(df, x_col, y_col = 'DateTime') {
  leaflet(data = df) %>%
    addTiles() %>%
    addCircles(
      lng = ~Longitude, 
      lat = ~Latitude,
      color = ~colorNumeric('viridis', df[[x_col]])(df[[x_col]]),
      popup = ~paste0('<b>', x_col, ':</b> ', df[[x_col]], '<br>',
                      '<b>', y_col, ':</b> ', df[[y_col]])
    ) %>%
    addLegend('bottomright', 
              pal = colorNumeric('viridis', df[[x_col]]), 
              values = df[[x_col]],
              title = x_col,
              opacity = 1)
}

analyte_cols <- colnames(df_final)[!colnames(df_final) %in% c('Longitude', 'Latitude', 'DateTime')]

for (col in analyte_cols) {
  print(create_map(df_final, col))
}
```




